{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define network(class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SCAE(object):\n",
    "    \"\"\"\n",
    "    Stacked Convolutional AutoEncoder\n",
    "    \"\"\"\n",
    "    def __init__(self, mode='train', batch_size= 128, learning_rate = 0.0002):\n",
    "        self.mode = mode\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.input_array = tf.placeholder(tf.float32, [None, 32, 32, 3], 'input_image')\n",
    "        self.reconstruct = self.build_model()\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.reconstruct - self.input_array))\n",
    "\n",
    "    def Encoder(self, input_array, reuse = False):\n",
    "        # input : (batch_size, 32, 32, 3)\n",
    "        with tf.variable_scope('Encoder', reuse=reuse):\n",
    "            with slim.arg_scope([slim.conv2d], padding='SAME', activation_fn=None, \n",
    "                                stride = 2, weights_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "                with slim.arg_scope([slim.batch_norm], decay=0.95, center=True, scale=True,\n",
    "                                    updates_collections = None, activation_fn=tf.nn.relu,\n",
    "                                    is_training =(self.mode =='train' or self.mode=='pretrain')):\n",
    "                    net = slim.conv2d(input_array, 6, [3,3], scope='e_conv1') # (batch_size, 16, 16, 2)\n",
    "                    net = slim.batch_norm(net, scope='e_bn1')\n",
    "                    net = slim.conv2d(net, 12, [3,3], scope='e_conv2') # (batch_size, 8, 8, 12)\n",
    "                    net = slim.batch_norm(net,activation_fn=tf.nn.tanh, scope='e_bn2')\n",
    "                    if self.mode == 'pretrain':\n",
    "                        net = slim.conv2d(net, 2, [1,1],  scope='out')\n",
    "                        net = slim.flatten(net)\n",
    "                    return net\n",
    "\n",
    "    def Decoder(self, z, reuse=False):\n",
    "        # z : (batch_size, 8, 8, 12)\n",
    "        with tf.variable_scope('Decoder', reuse=reuse):\n",
    "            with slim.arg_scope([slim.conv2d_transpose], padding='SAME', activation_fn=None,\n",
    "                               stride = 2, weights_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "                with slim.arg_scope([slim.batch_norm], decay = 0.95, center=True, scale=True,\n",
    "                                    updates_collections = None, activation_fn=tf.nn.relu,\n",
    "                                    is_training = (self.mode == 'train')):\n",
    "                    net = slim.conv2d_transpose(z, 6, [3,3], scope='d_conv1') # (batch_size, 16, 16, 6)\n",
    "                    net = slim.batch_norm(net,scope='d_bn1')\n",
    "                    net = slim.conv2d_transpose(net, 3, [3,3], scope='d_conv2') # (batch_size, 32, 32, 3)\n",
    "                    net = slim.batch_norm(net,activation_fn = tf.nn.tanh,scope='d_bn9')\n",
    "                    return net\n",
    "                    \n",
    "    def build_model(self):\n",
    "        self.embedding =self.Encoder(self.input_array)\n",
    "        self.recon = self.Decoder(self.embedding)\n",
    "        return self.recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, model, batch_size = 128, \n",
    "                 train_iter = 10000,\n",
    "                 log_dir = 'logs', \n",
    "                 model_save_path='model',\n",
    "                 test_model='model/train-9500'):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.train_iter = train_iter\n",
    "        self.log_dir = log_dir\n",
    "        self.model_save_path = model_save_path\n",
    "        self.test_model = test_model\n",
    "        self.config = tf.ConfigProto(log_device_placement=True)\n",
    "        self.config.gpu_options.allow_growth=True\n",
    "        self.data = np.load('image_32.npy').item()\n",
    "    \n",
    "    def batch_generator(self, shuffle = True):\n",
    "        id_ = np.array(list(self.data.keys()))\n",
    "        \n",
    "        data_size = len(id_)\n",
    "        num_batches_per_epoch = int(data_size/ self.batch_size)+1\n",
    "        \n",
    "        for epoch in range(num_batches_per_epoch):\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                id_shuffle = id_[shuffle_indices]\n",
    "            else:\n",
    "                id_shuffle = id_[shuffle_indices]\n",
    "                \n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = (batch_num +1) * batch_size\n",
    "                \n",
    "                if end_index < data_size:\n",
    "                    tmp_list = []\n",
    "                    id_batch = id_shuffle[start_index:end_index]\n",
    "                    for i in id_batch:\n",
    "                        tmp_list.append(self.data[i])\n",
    "                    yield tmp_list\n",
    "\n",
    "    def train(self):\n",
    "        with tf.Graph().as_default():\n",
    "            sess = tf.Session(config = self.config)\n",
    "            with sess.as_default():\n",
    "                scae = self.model\n",
    "#                 scae.build_model()\n",
    "        \n",
    "                if tf.gfile.Exists(self.log_dir):\n",
    "                    tf.gfile.DeleteRecursively(self.log_dir)\n",
    "                tf.gfile.MakeDirs(self.log_dir)\n",
    "                \n",
    "                global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "                opt = tf.train.AdamOptimizer(scae.learning_rate)\n",
    "                optimizer = opt.minimize(scae.loss, global_step = global_step)\n",
    "                \n",
    "                # omit varibale for tensorborad (about summary)\n",
    "                \n",
    "                saver = tf.train.Saver(tf.global_variables(), max_to_keep = 4)\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                batch_train = self.batch_generator()\n",
    "                \n",
    "                for data in batch_train:\n",
    "                    feed_dict = {scae.input_array : data}\n",
    "                    current_step = sess.run(global_step , feed_dict = feed_dict)\n",
    "                    optimizer.run(feed_dict = feed_dict)\n",
    "                    if current_step % 100 == 0:\n",
    "                        print(\"step: {}\".format(current_step))\n",
    "                        print(\"==validation start==\")\n",
    "                        batch_val = self.batch_generator()\n",
    "\n",
    "                        losses = []\n",
    "                        for val_data in batch_val:\n",
    "                            feed_dict = {scae.input_array : val_data}\n",
    "                            l = scae.loss.eval(feed_dict = feed_dict)\n",
    "                            losses.append(l)\n",
    "                        print(\"Mean loss  = \" + str(sum(losses)/ len(losses)))\n",
    "                        saver.save(sess, save_path = self.model_save_path, global_step = current_step)\n",
    "                        print(\"==training==\")\n",
    "                        \n",
    "                        \n",
    "                if(step+1) %500 == 0:\n",
    "                    loss = sess.run(model.loss, feed_dict)\n",
    "                    test_loss = sess.run(model.loss, feed_dict={model.spectrogram: test_spec})\n",
    "                    print(\"Step: [%d/%d] loss: [%.3f] test loss: [%.3f]\"%(step+1, self.train_iter, loss, test_loss))\n",
    "                    saver.save(sess, os.path.join(self.model_save_path, 'train'), global_step = step+1)\n",
    "                    print('train-%d saved..!'%(step+1))\n",
    "                    \n",
    "\n",
    "#     def test(self):\n",
    "#         model = self.model\n",
    "#         model.build_model()\n",
    "        \n",
    "#         with tf.Session(config = self.config) as sess:\n",
    "#             print(\"loading test model\")\n",
    "#             saver = tf.train.Saver()\n",
    "#             saver.restore(sess, self.test_model)\n",
    "\n",
    "#             for i in range(5):\n",
    "#                 batch_spec, _ = self.batch_fft(10)\n",
    "#                 feed_dict = {model.test_spectrogram : batch_spec}\n",
    "#                 sample_batch_recon = sess.run(model.test_recon, feed_dict)\n",
    "#                 fig, axs = plt.subplots(2, 5, figsize=(15, 4))\n",
    "#                 for example_i in range(5):\n",
    "#                     axs[0][example_i].matshow(np.reshape(batch_spec[example_i, :], (512, 512))) #, cmap=plt.get_cmap('gray'))\n",
    "#                     axs[1][example_i].matshow(np.reshape(sample_batch_recon[example_i, :], (512,512))) #, cmap=plt.get_cmap('gray'))\n",
    "#                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-60b6c5e5da90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-15c39d1a8899>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'global_step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# omit varibale for tensorborad (about summary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to optimize.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0mvar_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     grads = gradients.gradients(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to optimize."
     ]
    }
   ],
   "source": [
    "model = SCAE('train', learning_rate=0.005, batch_size = 128)\n",
    "solver = Solver(model)\n",
    "    \n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SCAE('test', learning_rate=0.0003, batch_size = 16)\n",
    "solver = Solver(model)\n",
    "    \n",
    "solver.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
